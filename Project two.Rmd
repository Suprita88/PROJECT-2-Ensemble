---
title: "Project 2"
author: "Suprita Bhomkar"
date: "3/13/2021"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
#install.packages("mlbench")
require(mlbench)
```

#The goal of this R program is to use several classification models for the Breast Cancer dataset and
#then combining the output from the models in an ensemble fashion. 

#Load Data
```{r}
data(BreastCancer)
BreastCancer <- na.omit(BreastCancer) 
BreastCancer$Id <- NULL #remove the ID column which is not required in the models

```

1. SVM
```{r}
library(e1071)
mysvm <- svm(Class ~ ., BreastCancer) #model
mysvm.pred <- predict(mysvm, BreastCancer)#Predict
table(mysvm.pred,BreastCancer$Class)
```

2. NaiyeBayes
```{r}
library(klaR)
mynb <- NaiveBayes(Class ~ ., BreastCancer)
mynb.pred <- predict(mynb,BreastCancer)#Predict
table(mynb.pred$class,BreastCancer$Class)
```

3. Neural Net
```{r}
library(nnet)
mynnet <- nnet(Class ~ ., BreastCancer, size=1)
mynnet.pred <- predict(mynnet,BreastCancer,type="class")#Predict
table(mynnet.pred,BreastCancer$Class)
```

4. Decision trees
```{r}
library(rpart)
mytree <- rpart(Class ~ ., BreastCancer)
summary(mytree)
mytree.pred <- predict(mytree,BreastCancer,type="class")#Predict
table(mytree.pred,BreastCancer$Class)
```

5. Leave-1-Out Cross Validation (LOOCV)
```{r}
ans <- numeric(length(BreastCancer[,1]))
for (i in 1:length(BreastCancer[,1])) {
  mytree <- rpart(Class ~ ., BreastCancer[-i,])
  mytree.pred <- predict(mytree,BreastCancer[i,],type="class") #Predict
  ans[i] <- mytree.pred
}
ans <- factor(ans,labels=levels(BreastCancer$Class))
table(ans,BreastCancer$Class)
# The same as above in this case
```

6. Quadratic Discriminant Analysis (error)
```{r}
BreastCancer1<-BreastCancer

#convert  all columsn to numeric. using factors does nto run model
for (i in 1:dim(BreastCancer)[2])
{
  BreastCancer1[,i]<-as.numeric(BreastCancer1[,i])
}

library(MASS)
myqda <- qda(Class ~ .,data= BreastCancer1) #Model QDA
myqda.pred <- predict(myqda, BreastCancer1) #Predict
table(myqda.pred$class,BreastCancer$Class)

```

7. Regularised Discriminant Analysis
```{r}
library(klaR)
myrda <- rda(Class ~ ., BreastCancer) #model RDa
myrda.pred <- predict(myrda, BreastCancer) #Predict
table(myrda.pred$class,BreastCancer$Class)

```

8.Random Forests
```{r}
library(randomForest)
myrf <- randomForest(Class ~ .,BreastCancer)#Model
myrf.pred <- predict(myrf, BreastCancer)#predict
table(myrf.pred, BreastCancer$Class)
```


# Combining the predictions from individual models into one dataframe for easy comparison and majority vote.

```{r}
#install.packages("mclust")
library(mclust)
library(sjmisc)

mynnet.pred<-as.factor(mynnet.pred)# as this output was character, convert to factor as output of other methods

#combine outputs into a dataframe
final.df<- data.frame("SVM"=mysvm.pred, "Random"=myrf.pred,"Regularized"= myrda.pred$class, "LOOCV"=ans,"DecisionTree"=mytree.pred, "Neural Net"=mynnet.pred,"NaiyeBayes"=mynb.pred$class )

#convert all the values to numeric
for (i in 1:ncol(final.df)){
  final.df[,i]<-as.numeric(final.df[,i])
}
```

# MAJORITY VOTE
```{r}
library(caret)
#As the levels are 1 and 2, we check if the value is above 8 then call it 2 otherwise 1
final.df$MajorityVote<- ifelse(rowSums(final.df)>8,2,1)
head(final.df,10)# validate majority rule
final.df$MajorityVoteCat<-ifelse(final.df$MajorityVote==1,"benign","malignant")

confusionMatrix(as.factor(BreastCancer$Class),as.factor(final.df$MajorityVoteCat))
#After the Ensemble method, we can see a high accuracy of 97.2 percent.
```

